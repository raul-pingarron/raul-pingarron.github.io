---
layout: post
title: Cómo utilizar una versión distinta de Python en Spark
date: '2019-04-06T23:55:00.000+02:00'
author: Raul Pingarron
tags:
- BigData
modified_time: '2019-08-24T00:20:27.787+02:00'
thumbnail: https://1.bp.blogspot.com/-w7jL9-mSoJw/XWBfl1G_0SI/AAAAAAAAAc8/gnHyODlbjLQgZTkhWPjsZ1ToJPJnvaExwCLcBGAs/s72-c/pyspark.jpeg
blogger_id: tag:blogger.com,1999:blog-5861200429574691520.post-131737448507829861
blogger_orig_url: https://raul.pingarron.net/2019/06/utilizar-version-distinta-de-python-en-spark.html
---

<br /><div class="separator" style="clear: both; text-align: center;"></div><div class="separator" style="clear: both; text-align: center;"><a href="https://1.bp.blogspot.com/-w7jL9-mSoJw/XWBfl1G_0SI/AAAAAAAAAc8/gnHyODlbjLQgZTkhWPjsZ1ToJPJnvaExwCLcBGAs/s1600/pyspark.jpeg" imageanchor="1" style="margin-left: 1em; margin-right: 1em; text-align: center;"><img border="0" data-original-height="232" data-original-width="410" height="113" src="https://1.bp.blogspot.com/-w7jL9-mSoJw/XWBfl1G_0SI/AAAAAAAAAc8/gnHyODlbjLQgZTkhWPjsZ1ToJPJnvaExwCLcBGAs/s200/pyspark.jpeg" width="200" /></a></div><div class="MsoNormal">Las últimas versiones de Spark 2 son capaces de ejecutar código de cualquier versión de Python igual o superior a 2.7 y 3.4 (el soporte para Python 2.6 fue eliminado a partir de Spark 2.2.0). <o:p></o:p></div><div class="MsoNormal">Por defecto, en algunas situaciones, PySpark utiliza los ejecutables binarios de Python 2.7 tanto en el <i>driver</i> como en los <i style="mso-bidi-font-style: normal;">workers</i> o ejecutores, ya que ésta suele ser la versión predeterminada de Python que se puede encontrar en bastantes de las distribuciones de sistemas operativos Linux con soporte empresarial.</div><div class="MsoNormal"><o:p></o:p></div><div class="MsoNormal"><br /></div><div class="MsoNormal">Pero ¿y si queremos utilizar otra versión distinta de Python en Spark?.&nbsp;</div><div class="MsoNormal"><br /></div><div class="MsoNormal">Por suerte, en Spark es posible instalar y usar múltiples versiones de Python; es tan simple como desplegar la versión requerida de Python tanto en el servidor o nodo que ejecuta el programa <i>driver</i> como en el nodo <i style="mso-bidi-font-style: normal;">master</i> así como en todos nodos <i style="mso-bidi-font-style: normal;">workers</i> o <i style="mso-bidi-font-style: normal;">executors</i> y luego usar las variables de entorno de Spark para especificar qué versión usar. <o:p></o:p></div><div class="MsoNormal"><br /></div><div class="MsoNormal">Para desplegar la versión requerida de Python podemos, por ejemplo, instalar Anaconda en el servidor o nodo donde tenemos instalado el cliente de Spark así como en todos los nodos que son <i style="mso-bidi-font-style: normal;">workers</i> o ejecutores de Spark en nuestro clúster (por ejemplo, podemos instalar Anaconda3 en todos los nodos implicados bajo&nbsp;<span style="font-family: &quot;consolas&quot;; font-size: 11pt;">/opt/anaconda3</span>). Para indicar a Spark que esta nueva versión instalada de Python será la utilizada, simplemente hay que configurar o establecer la variable de entorno&nbsp;<span style="font-family: &quot;consolas&quot;; font-size: 11pt;"><b>PYSPARK_PYTHON</b></span>&nbsp;en el nodo cliente que envía el trabajo de Spark (desde donde se lanza el&nbsp;<span style="font-family: &quot;consolas&quot;; font-size: 11pt;">spark-submit</span>&nbsp;o desde donde se arranca la Shell de PySpark).&nbsp;</div><div class="MsoNormal">Esto se puede lograr fácilmente añadiendo lo siguiente al&nbsp;<span style="font-family: &quot;consolas&quot;; font-size: 11pt;">.bashrc&nbsp;</span>del perfil del usuario:<br /><o:p></o:p></div><div class="MsoNormal"><br /></div><div class="MsoNormal"><div class="MsoNormal"><span style="font-family: &quot;consolas&quot;;"><b>export PYSPARK_PYTHON="/opt/anaconda3/bin/python3"</b><o:p></o:p></span></div></div><div class="MsoNormal"><br /></div><div class="MsoNormal">Para más información echa un vistazo a <span style="mso-spacerun: yes;">&nbsp;</span><a href="https://spark.apache.org/docs/latest/configuration.html#environment-variables" rel="nofollow" target="_blank">https://spark.apache.org/docs/latest/configuration.html#environment-variables</a><o:p></o:p></div><div class="MsoNormal"><br /></div><div class="MsoNormal">Siguiendo nuestro ejemplo, una vez instalado Anaconda3 en los nodos afectados y habiendo cambiado la variable de entorno (no olvidar ejecutar antes un&nbsp;<span style="font-family: &quot;consolas&quot;; font-size: 11.0pt; line-height: 107%;">source .bashrc</span>), comprobamos el funcionamiento iniciando la shell PySpark:<o:p></o:p></div><br /><br /><div class="separator" style="clear: both; text-align: center;"><a href="https://1.bp.blogspot.com/-rK1sG1qu2UY/XWBfl5zMAwI/AAAAAAAAAdI/7PuIpZtacUgYngUlOjSGkZet8UitC5RGwCEwYBhgL/s1600/PySpark_shell.JPG" imageanchor="1" style="margin-left: 1em; margin-right: 1em;"><img border="0" data-original-height="263" data-original-width="635" src="https://1.bp.blogspot.com/-rK1sG1qu2UY/XWBfl5zMAwI/AAAAAAAAAdI/7PuIpZtacUgYngUlOjSGkZet8UitC5RGwCEwYBhgL/s1600/PySpark_shell.JPG" /></a></div><br />